{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notebook created by: Gabriele Sottocornola\n",
    "for the M.Sc. class of Data & Text Mining\n",
    "'''\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_gensim_topics(topics_list):\n",
    "    '''\n",
    "    Function to reformat the word-topic list given the output of gensim lda\n",
    "    '''\n",
    "    topics_report_string = ''\n",
    "    \n",
    "    for t in topics_list:        \n",
    "        topic_id = str(t[0])\n",
    "        t_word_tokens = t[1].split('+')        \n",
    "        topics_report_string += 'Topic ' + topic_id + ', '\n",
    "        \n",
    "        for w in t_word_tokens:            \n",
    "            topics_report_string += w.split('*')[1].strip() + ', '\n",
    "            \n",
    "        topics_report_string = topics_report_string[:-2] + '\\n'\n",
    "    \n",
    "    return topics_report_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenization_preprocessing(corpus_list):\n",
    "    '''\n",
    "    Function to apply some standard preprocessing to a list of documents in a corpus.\n",
    "    The preprocessing includes: split each document into a list of tokens, remove english stopwords,\n",
    "    remove alphanumerical tokens shorter that 3 chars.\n",
    "    Return a list of documents, each represented as a list of tokens\n",
    "    '''\n",
    "    #set tokenizer and stopwords filter\n",
    "    tokenizer = RegexpTokenizer('[–\\\\w\\\\+\\\\-\\\\*′α-ωΑ-Ω]+')\n",
    "    en_stop = stop_words.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    doc_tokens_list = list() #list of tokens that represent each document\n",
    "\n",
    "    for document in corpus_list:\n",
    "        lower_doc = document.lower() #set all the document string to lowercase\n",
    "        tokens = tokenizer.tokenize(lower_doc) #tokenize the document string\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop] #filter out english stopwords\n",
    "        final_tokens = [i for i in stopped_tokens if (len(i) > 2) and re.search('[a-zA-Z]', i)] #filter out tokens with len less than 3 and numbers\n",
    "\n",
    "        doc_tokens_list.append(final_tokens)\n",
    "        \n",
    "    return doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words_list(doc_tokens_list, top_n_words=100):\n",
    "    '''\n",
    "    Function to get the list of the top_n_words most frequent words inside the corpus.\n",
    "    Return a tuple with a list of the most frequent word labels and frequencies\n",
    "    '''\n",
    "    dictionary = corpora.dictionary.Dictionary(doc_tokens_list) #dictionary of words extracted from the corpus (list of tokens)\n",
    "    bow = dictionary.doc2bow([token for doc in doc_tokens_list for token in doc])\n",
    "    bow.sort(key=lambda x:x[1], reverse=True) #bow rapresentation of the dictionary sorted descending by frequency\n",
    "    \n",
    "    word_label_list = [dictionary[bow[x][0]] for x in range(top_n_words)]\n",
    "    word_freq_list = [bow[x][1] for x in range(top_n_words)]\n",
    "    \n",
    "    return (word_label_list, word_freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_doc_tokens(doc_tokens_list):\n",
    "    '''\n",
    "    Function to stem all the word tokens provided by the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of stemmed tokens\n",
    "    '''\n",
    "    stem_doc_tokens_list = list()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens_list]\n",
    "        stem_doc_tokens_list.append(stemmed_tokens)\n",
    "        \n",
    "    return stem_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_common_words(doc_tokens_list, list_filter_words):\n",
    "    '''\n",
    "    Function to filter out the most common words provided by list_filter_words from the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of filtered tokens\n",
    "    '''\n",
    "    filtered_doc_tokens_list = list()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:        \n",
    "        filtered_tokens_list = [token for token in tokens_list if not(token in list_filter_words)]\n",
    "        filtered_doc_tokens_list.append(filtered_tokens_list)\n",
    "    \n",
    "    return filtered_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_model_evaluation_pipeline(doc_tokens_list):\n",
    "    \n",
    "    '''\n",
    "    Function that allow to apply a set of different topic models (with different parameters)\n",
    "    and evaluate the coherence of the models according to qualitative and quantitive measures (PMI-based)\n",
    "    '''\n",
    "    dictionary = corpora.dictionary.Dictionary(doc_tokens_list) #dictionary of words extracted from the corpus (list of tokens)\n",
    "    bow_corpus = [dictionary.doc2bow(tokens) for tokens in doc_tokens_list] #gensim bag-of-words representation of corpus\n",
    "    \n",
    "    #evaluation of the coherence and quality of different topic models\n",
    "    for nt in [5, 10, 20, 30, 50]:\n",
    "\n",
    "        print('GENSIM LDA WITH {} TOPICS \\n'.format(nt))\n",
    "        \n",
    "        #apply LDA models and extract top10 most representative words for each topic\n",
    "        ldamodel = LdaModel(bow_corpus, num_topics=nt, id2word=dictionary, alpha='asymmetric', minimum_probability=0.0001, iterations=1000)\n",
    "        topics = ldamodel.print_topics(num_topics=-1, num_words=10)\n",
    "\n",
    "        print('Top10 words for each topic:\\n')\n",
    "        print(reformat_gensim_topics(topics))\n",
    "        \n",
    "        #measure of topic coherence based on PMI (more details here: http://qpleple.com/topic-coherence-to-evaluate-topic-models/)\n",
    "        c_uci = CoherenceModel(model=ldamodel, texts=doc_tokens_list, dictionary=dictionary, coherence='c_uci', topn=30)   \n",
    "        u_mass = CoherenceModel(model=ldamodel, corpus=bow_corpus, coherence='u_mass', topn=30)\n",
    "\n",
    "        print('UCI coherence for {} topics LDA = {}'.format(nt, c_uci.get_coherence()))\n",
    "        print('UMASS coherence for {} topics LDA = {}'.format(nt, u_mass.get_coherence()))\n",
    "\n",
    "        print('\\n******************************************************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read textual documents from file\n",
    "documents_path = '.\\\\data\\\\AssociatedPress.txt'\n",
    "with open(documents_path, 'r', encoding='utf-8') as doc_f:\n",
    "    corpus_list = doc_f.readlines()\n",
    "corpus_list = [re.sub('\\d+\\|\\|', '', doc) for doc in corpus_list] #list of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize documents and remove stopwords\n",
    "doc_tokens_list = tokenization_preprocessing(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stem word tokens\n",
    "stem_doc_tokens_list = stem_doc_tokens(doc_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove most frequent words\n",
    "frequent_words_list = get_common_words_list(stem_doc_tokens_list, 50)[0]\n",
    "final_doc_tokens_list = filter_common_words(stem_doc_tokens_list, frequent_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENSIM LDA WITH 5 TOPICS \n",
      "\n",
      "Top10 words for each topic:\n",
      "\n",
      "Topic 0, \"democrat\", \"dollar\", \"dukaki\", \"vote\", \"rate\", \"bank\", \"south\", \"late\", \"leader\", \"campaign\"\n",
      "Topic 1, \"case\", \"charg\", \"defens\", \"committe\", \"program\", \"offer\", \"comput\", \"judg\", \"north\", \"want\"\n",
      "Topic 2, \"cent\", \"did\", \"drug\", \"attorney\", \"charg\", \"gener\", \"way\", \"dress\", \"campaign\", \"mecham\"\n",
      "Topic 3, \"kill\", \"south\", \"monday\", \"militari\", \"right\", \"death\", \"home\", \"servic\", \"author\", \"oper\"\n",
      "Topic 4, \"sale\", \"tax\", \"stock\", \"share\", \"depart\", \"gener\", \"industri\", \"increas\", \"foreign\", \"administr\"\n",
      "\n",
      "UCI coherence for 5 topics LDA = -0.4383156661665164\n",
      "UMASS coherence for 5 topics LDA = -2.799180186770827\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "GENSIM LDA WITH 10 TOPICS \n",
      "\n",
      "Top10 words for each topic:\n",
      "\n",
      "Topic 0, \"senat\", \"committe\", \"germani\", \"meet\", \"right\", \"mecham\", \"world\", \"attorney\", \"nordstrom\", \"union\"\n",
      "Topic 1, \"cent\", \"sale\", \"iraq\", \"higher\", \"oil\", \"lower\", \"econom\", \"soybean\", \"economi\", \"export\"\n",
      "Topic 2, \"south\", \"aid\", \"children\", \"black\", \"africa\", \"studi\", \"health\", \"blood\", \"dress\", \"infect\"\n",
      "Topic 3, \"south\", \"kill\", \"prison\", \"elect\", \"releas\", \"moslem\", \"death\", \"victim\", \"author\", \"man\"\n",
      "Topic 4, \"rate\", \"increas\", \"school\", \"share\", \"stock\", \"sale\", \"yeutter\", \"bank\", \"declin\", \"order\"\n",
      "Topic 5, \"east\", \"german\", \"research\", \"program\", \"west\", \"chang\", \"aid\", \"go\", \"want\", \"germani\"\n",
      "Topic 6, \"dukaki\", \"democrat\", \"york\", \"administr\", \"campaign\", \"vote\", \"gener\", \"poll\", \"republican\", \"worker\"\n",
      "Topic 7, \"pac\", \"defens\", \"money\", \"campaign\", \"program\", \"tax\", \"budget\", \"elect\", \"total\", \"fiscal\"\n",
      "Topic 8, \"dollar\", \"drug\", \"charg\", \"london\", \"late\", \"owen\", \"case\", \"yen\", \"militari\", \"friday\"\n",
      "Topic 9, \"late\", \"dollar\", \"counti\", \"monday\", \"bank\", \"bid\", \"close\", \"gene\", \"dump\", \"london\"\n",
      "\n",
      "UCI coherence for 10 topics LDA = -0.9472111344246259\n",
      "UMASS coherence for 10 topics LDA = -3.5311450012368457\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "GENSIM LDA WITH 20 TOPICS \n",
      "\n",
      "Top10 words for each topic:\n",
      "\n",
      "Topic 0, \"south\", \"prison\", \"talk\", \"pac\", \"charg\", \"defens\", \"gener\", \"convict\", \"attorney\", \"rebel\"\n",
      "Topic 1, \"south\", \"africa\", \"maxwel\", \"iraq\", \"charg\", \"macmillan\", \"iran\", \"gunter\", \"case\", \"pay\"\n",
      "Topic 2, \"nordstrom\", \"labor\", \"dukaki\", \"lafayett\", \"elect\", \"charg\", \"share\", \"offer\", \"willi\", \"kill\"\n",
      "Topic 3, \"dukaki\", \"jackson\", \"york\", \"panama\", \"bank\", \"democrat\", \"campaign\", \"administr\", \"thompson\", \"vice\"\n",
      "Topic 4, \"maung\", \"act\", \"congress\", \"gener\", \"flight\", \"end\", \"home\", \"tax\", \"gull\", \"union\"\n",
      "Topic 5, \"abort\", \"cent\", \"defens\", \"plant\", \"higher\", \"plane\", \"contract\", \"anti-abort\", \"saudi\", \"soybean\"\n",
      "Topic 6, \"butterfli\", \"releas\", \"film\", \"novemb\", \"order\", \"ireland\", \"hepat\", \"china\", \"car\", \"movi\"\n",
      "Topic 7, \"kill\", \"monday\", \"cent\", \"dump\", \"mile\", \"south\", \"ship\", \"rig\", \"air\", \"news\"\n",
      "Topic 8, \"counti\", \"flood\", \"water\", \"area\", \"servic\", \"ohio\", \"river\", \"china\", \"retail\", \"home\"\n",
      "Topic 9, \"dollar\", \"bank\", \"late\", \"aid\", \"germani\", \"london\", \"west\", \"friday\", \"yen\", \"close\"\n",
      "Topic 10, \"owen\", \"poll\", \"drug\", \"cocain\", \"public\", \"black\", \"video\", \"barahona\", \"mexican\", \"case\"\n",
      "Topic 11, \"comput\", \"point\", \"rubi\", \"share\", \"york\", \"souter\", \"sale\", \"industri\", \"senat\", \"financi\"\n",
      "Topic 12, \"worker\", \"studi\", \"insur\", \"wage\", \"oper\", \"ranch\", \"island\", \"corroon\", \"communist\", \"cdi\"\n",
      "Topic 13, \"vote\", \"campaign\", \"democrat\", \"committe\", \"senat\", \"republican\", \"primari\", \"voter\", \"elect\", \"dukaki\"\n",
      "Topic 14, \"tax\", \"militari\", \"school\", \"washington\", \"robertson\", \"newspap\", \"minist\", \"law\", \"right\", \"polit\"\n",
      "Topic 15, \"document\", \"nea\", \"art\", \"endow\", \"case\", \"frohnmay\", \"gesel\", \"north\", \"secret\", \"east\"\n",
      "Topic 16, \"dress\", \"venu\", \"pac\", \"jupit\", \"home\", \"just\", \"vote\", \"go\", \"man\", \"come\"\n",
      "Topic 17, \"mecham\", \"law\", \"tax\", \"anderson\", \"program\", \"dixon\", \"deficit\", \"incom\", \"senat\", \"milstead\"\n",
      "Topic 18, \"stock\", \"share\", \"averag\", \"japan\", \"rate\", \"econom\", \"agreement\", \"sale\", \"rose\", \"industri\"\n",
      "Topic 19, \"order\", \"store\", \"sale\", \"busi\", \"defens\", \"hunthausen\", \"increas\", \"food\", \"good\", \"john\"\n",
      "\n",
      "UCI coherence for 20 topics LDA = -2.425712083777194\n",
      "UMASS coherence for 20 topics LDA = -5.316717859307772\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "GENSIM LDA WITH 30 TOPICS \n",
      "\n",
      "Top10 words for each topic:\n",
      "\n",
      "Topic 0, \"dukaki\", \"primari\", \"just\", \"think\", \"campaign\", \"go\", \"way\", \"film\", \"poll\", \"robertson\"\n",
      "Topic 1, \"iraqi\", \"north\", \"kill\", \"iran\", \"refuge\", \"thompson\", \"iraq\", \"south\", \"continent\", \"hunthausen\"\n",
      "Topic 2, \"dukaki\", \"south\", \"campaign\", \"administr\", \"want\", \"document\", \"senat\", \"reagan\", \"committe\", \"north\"\n",
      "Topic 3, \"tax\", \"presley\", \"incom\", \"cassett\", \"deduct\", \"novemb\", \"heath\", \"octob\", \"black\", \"estat\"\n",
      "Topic 4, \"nordstrom\", \"poll\", \"vote\", \"rate\", \"cuomo\", \"adopt\", \"store\", \"senat\", \"committe\", \"pay\"\n",
      "Topic 5, \"south\", \"africa\", \"share\", \"maxwel\", \"yeutter\", \"stock\", \"talk\", \"hyundai\", \"point\", \"farm\"\n",
      "Topic 6, \"venu\", \"drug\", \"ohio\", \"bridal\", \"case\", \"test\", \"galileo\", \"style\", \"counti\", \"depart\"\n",
      "Topic 7, \"german\", \"germani\", \"west\", \"east\", \"econom\", \"talk\", \"union\", \"european\", \"meet\", \"unif\"\n",
      "Topic 8, \"vote\", \"democrat\", \"dump\", \"mohawk\", \"garbag\", \"elector\", \"primari\", \"germani\", \"just\", \"trash\"\n",
      "Topic 9, \"book\", \"barr\", \"award\", \"novel\", \"mobley\", \"zambia\", \"fiction\", \"launch\", \"atkin\", \"booster\"\n",
      "Topic 10, \"pac\", \"defens\", \"senat\", \"committe\", \"bank\", \"money\", \"budget\", \"cost\", \"fund\", \"program\"\n",
      "Topic 11, \"infect\", \"viru\", \"aid\", \"diseas\", \"number\", \"estim\", \"develop\", \"cdc\", \"test\", \"plant\"\n",
      "Topic 12, \"dollar\", \"stock\", \"sale\", \"late\", \"rose\", \"rate\", \"london\", \"bank\", \"share\", \"bid\"\n",
      "Topic 13, \"car\", \"bride\", \"anderson\", \"coron\", \"tree\", \"judg\", \"hostag\", \"daughter\", \"tower\", \"wife\"\n",
      "Topic 14, \"cent\", \"school\", \"lower\", \"corn\", \"higher\", \"soybean\", \"board\", \"bushel\", \"hasselbr\", \"prom\"\n",
      "Topic 15, \"elect\", \"leader\", \"minist\", \"chang\", \"hudson\", \"kill\", \"charg\", \"student\", \"way\", \"social\"\n",
      "Topic 16, \"art\", \"flood\", \"inch\", \"counti\", \"greyhound\", \"european\", \"west\", \"friday\", \"oxford\", \"rain\"\n",
      "Topic 17, \"lafayett\", \"venu\", \"ireland\", \"joan\", \"jupit\", \"french\", \"gravley\", \"dollar\", \"car\", \"british\"\n",
      "Topic 18, \"dress\", \"antiqu\", \"saudi\", \"troop\", \"sasser\", \"go\", \"broadcast\", \"deadlin\", \"desert\", \"dukaki\"\n",
      "Topic 19, \"mecham\", \"senat\", \"attorney\", \"gener\", \"alleg\", \"news\", \"milstead\", \"impeach\", \"governor\", \"threat\"\n",
      "Topic 20, \"owen\", \"children\", \"jackson\", \"drug\", \"york\", \"arafat\", \"aid\", \"doctor\", \"spain\", \"barahona\"\n",
      "Topic 21, \"japan\", \"increas\", \"econom\", \"world\", \"agreement\", \"export\", \"continu\", \"industri\", \"intern\", \"rate\"\n",
      "Topic 22, \"turkish\", \"cdi\", \"cuellar\", \"perez\", \"clr\", \"holderman\", \"survey\", \"reimburs\", \"asylum-seek\", \"expens\"\n",
      "Topic 23, \"dixon\", \"drug\", \"panama\", \"blood\", \"hepat\", \"health\", \"aid\", \"cat\", \"viru\", \"sheehan\"\n",
      "Topic 24, \"case\", \"nea\", \"sentenc\", \"judg\", \"trial\", \"firm\", \"black\", \"busi\", \"convict\", \"endow\"\n",
      "Topic 25, \"order\", \"needl\", \"rubi\", \"cdi\", \"clr\", \"shark\", \"ranch\", \"categori\", \"superconduct\", \"good\"\n",
      "Topic 26, \"black\", \"south\", \"koch\", \"releas\", \"polit\", \"african\", \"mandela\", \"britain\", \"rep\", \"art\"\n",
      "Topic 27, \"tax\", \"comput\", \"increas\", \"electr\", \"econom\", \"oil\", \"agreement\", \"import\", \"averag\", \"incom\"\n",
      "Topic 28, \"abort\", \"right\", \"souter\", \"human\", \"elector\", \"senat\", \"iraq\", \"committe\", \"vote\", \"kuwait\"\n",
      "Topic 29, \"cent\", \"mash\", \"juli\", \"oil\", \"gallon\", \"bushel\", \"march\", \"usda\", \"futur\", \"soybean\"\n",
      "\n",
      "UCI coherence for 30 topics LDA = -2.862109564273807\n",
      "UMASS coherence for 30 topics LDA = -5.977238564706536\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      "GENSIM LDA WITH 50 TOPICS \n",
      "\n",
      "Top10 words for each topic:\n",
      "\n",
      "Topic 0, \"senat\", \"abort\", \"econom\", \"administr\", \"committe\", \"propos\", \"owen\", \"share\", \"right\", \"chang\"\n",
      "Topic 1, \"owen\", \"nato\", \"ross\", \"agenc\", \"offer\", \"leonard\", \"passeng\", \"art\", \"charg\", \"barahona\"\n",
      "Topic 2, \"dress\", \"bridal\", \"style\", \"wed\", \"bride\", \"popular\", \"skirt\", \"energi\", \"simmon\", \"color\"\n",
      "Topic 3, \"poll\", \"iraq\", \"worker\", \"vote\", \"gener\", \"agenc\", \"iraqi\", \"job\", \"foreign\", \"news\"\n",
      "Topic 4, \"comput\", \"nordstrom\", \"endow\", \"retail\", \"nea\", \"art\", \"continent\", \"store\", \"employe\", \"analyst\"\n",
      "Topic 5, \"kill\", \"moslem\", \"kashmir\", \"counti\", \"india\", \"area\", \"servic\", \"death\", \"bodi\", \"injur\"\n",
      "Topic 6, \"case\", \"viru\", \"human\", \"charg\", \"infect\", \"right\", \"counti\", \"aid\", \"creek\", \"dump\"\n",
      "Topic 7, \"agreement\", \"talk\", \"negoti\", \"agricultur\", \"export\", \"minist\", \"import\", \"farm\", \"subsidi\", \"japan\"\n",
      "Topic 8, \"victim\", \"hunthausen\", \"student\", \"home\", \"releas\", \"public\", \"estim\", \"cdc\", \"arrest\", \"island\"\n",
      "Topic 9, \"dixon\", \"children\", \"filipino\", \"ohio\", \"west\", \"counti\", \"home\", \"arrest\", \"popul\", \"store\"\n",
      "Topic 10, \"panama\", \"game\", \"kill\", \"student\", \"south\", \"troop\", \"don\", \"refuge\", \"elect\", \"soldier\"\n",
      "Topic 11, \"dress\", \"come\", \"just\", \"way\", \"pac\", \"navi\", \"money\", \"rais\", \"servic\", \"ship\"\n",
      "Topic 12, \"mandela\", \"releas\", \"south\", \"copi\", \"polit\", \"simon\", \"burma\", \"illeg\", \"rangoon\", \"viru\"\n",
      "Topic 13, \"elect\", \"africa\", \"militari\", \"south\", \"leader\", \"sale\", \"democrat\", \"news\", \"kill\", \"mr\"\n",
      "Topic 14, \"plane\", \"water\", \"oil\", \"gunter\", \"saudi\", \"gallon\", \"flood\", \"normal\", \"air\", \"fli\"\n",
      "Topic 15, \"cdi\", \"elector\", \"clr\", \"black\", \"chines\", \"corroon\", \"klerk\", \"china\", \"alpo\", \"deer\"\n",
      "Topic 16, \"race\", \"death\", \"salvadoran\", \"disarray\", \"doctor\", \"joya-martinez\", \"venezia\", \"live\", \"kill\", \"barcelona\"\n",
      "Topic 17, \"moslem\", \"gambl\", \"war\", \"coaster\", \"mohawk\", \"redesign\", \"pakistan\", \"die\", \"india\", \"kill\"\n",
      "Topic 18, \"west\", \"imperi\", \"coron\", \"emperor\", \"germani\", \"elector\", \"uss\", \"world\", \"hirohito\", \"palac\"\n",
      "Topic 19, \"overal\", \"user\", \"accid\", \"program\", \"octob\", \"rose\", \"busi\", \"novemb\", \"needl\", \"increas\"\n",
      "Topic 20, \"mecham\", \"macmillan\", \"maxwel\", \"venu\", \"galileo\", \"spacecraft\", \"senat\", \"jupit\", \"milstead\", \"earth\"\n",
      "Topic 21, \"attorney\", \"trial\", \"charg\", \"judg\", \"case\", \"alleg\", \"gener\", \"document\", \"law\", \"indict\"\n",
      "Topic 22, \"dukaki\", \"campaign\", \"democrat\", \"vote\", \"jackson\", \"primari\", \"republican\", \"voter\", \"presidenti\", \"bentsen\"\n",
      "Topic 23, \"adopt\", \"hose\", \"student\", \"school\", \"child\", \"cavazo\", \"famili\", \"children\", \"robert\", \"royal\"\n",
      "Topic 24, \"souter\", \"maung\", \"vote\", \"right\", \"union\", \"committe\", \"major\", \"leader\", \"shadysid\", \"york\"\n",
      "Topic 25, \"bank\", \"charg\", \"immun\", \"aid\", \"wheelchair\", \"steinberg\", \"convict\", \"program\", \"case\", \"gener\"\n",
      "Topic 26, \"stock\", \"share\", \"industri\", \"averag\", \"unchang\", \"index\", \"dow\", \"jone\", \"volum\", \"exchang\"\n",
      "Topic 27, \"tax\", \"rate\", \"budget\", \"increas\", \"deficit\", \"fuel\", \"incom\", \"administr\", \"leader\", \"economi\"\n",
      "Topic 28, \"corn\", \"hasselbr\", \"farmer\", \"yield\", \"crop\", \"bushel\", \"drought\", \"hearten\", \"acr\", \"herd\"\n",
      "Topic 29, \"drug\", \"gene\", \"develop\", \"cell\", \"research\", \"blood\", \"diseas\", \"human\", \"air\", \"iraq\"\n",
      "Topic 30, \"south\", \"africa\", \"stock\", \"korea\", \"north\", \"israel\", \"arafat\", \"korean\", \"share\", \"meet\"\n",
      "Topic 31, \"studi\", \"abus\", \"econom\", \"misunderstand\", \"drug\", \"human\", \"cuellar\", \"castaneda\", \"job\", \"displac\"\n",
      "Topic 32, \"hyundai\", \"order\", \"firm\", \"offer\", \"corp\", \"busi\", \"york\", \"share\", \"brokerag\", \"invest\"\n",
      "Topic 33, \"reagan\", \"world\", \"imf\", \"barrier\", \"america\", \"tape\", \"problem\", \"fund\", \"cancer-caus\", \"export\"\n",
      "Topic 34, \"rep\", \"lafayett\", \"black\", \"novel\", \"koch\", \"ive\", \"sanction\", \"circl\", \"mr\", \"memori\"\n",
      "Topic 35, \"yeutter\", \"campaign\", \"fair\", \"democrat\", \"candid\", \"elect\", \"primari\", \"japanes\", \"major\", \"contribut\"\n",
      "Topic 36, \"robertson\", \"anderson\", \"kidnap\", \"mice\", \"dinner\", \"continu\", \"conn\", \"cancer\", \"mad\", \"cell\"\n",
      "Topic 37, \"cent\", \"higher\", \"soybean\", \"lower\", \"corn\", \"futur\", \"bushel\", \"mohawk\", \"juli\", \"water\"\n",
      "Topic 38, \"champion\", \"default\", \"dukaki\", \"job\", \"case\", \"ton\", \"york\", \"public\", \"worker\", \"poll\"\n",
      "Topic 39, \"prison\", \"maizier\", \"german\", \"store\", \"reimburs\", \"news\", \"east\", \"lender\", \"chain\", \"medicaid\"\n",
      "Topic 40, \"film\", \"butterfli\", \"superconduct\", \"frank\", \"movi\", \"center\", \"fla\", \"fiction\", \"debut\", \"ala\"\n",
      "Topic 41, \"cent\", \"lower\", \"higher\", \"hepat\", \"soybean\", \"futur\", \"hugh\", \"bushel\", \"decemb\", \"sale\"\n",
      "Topic 42, \"plant\", \"school\", \"prom\", \"namphi\", \"air\", \"medic\", \"pollut\", \"emiss\", \"danc\", \"clean\"\n",
      "Topic 43, \"sale\", \"worker\", \"estat\", \"wage\", \"averag\", \"joint\", \"presley\", \"labor\", \"pay\", \"gain\"\n",
      "Topic 44, \"rubi\", \"bank\", \"willi\", \"plane\", \"foam\", \"financi\", \"hostag\", \"bomber\", \"interst\", \"provid\"\n",
      "Topic 45, \"dollar\", \"late\", \"london\", \"bid\", \"rate\", \"yen\", \"gold\", \"canadian\", \"rose\", \"close\"\n",
      "Topic 46, \"pac\", \"defens\", \"committe\", \"senat\", \"rubi\", \"gener\", \"money\", \"contractor\", \"panel\", \"contribut\"\n",
      "Topic 47, \"germani\", \"german\", \"east\", \"west\", \"unif\", \"benson\", \"kohl\", \"cuomo\", \"tower\", \"build\"\n",
      "Topic 48, \"barr\", \"palestinian\", \"guida\", \"jerusalem\", \"investig\", \"attorney\", \"term\", \"israel\", \"sentenc\", \"isra\"\n",
      "Topic 49, \"anc\", \"oil\", \"mandela\", \"leonard\", \"appl\", \"rig\", \"birth\", \"african\", \"count\", \"product\"\n",
      "\n",
      "UCI coherence for 50 topics LDA = -3.1673186446245656\n",
      "UMASS coherence for 50 topics LDA = -6.530131896186972\n",
      "\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply and evaluate LDA models\n",
    "topic_model_evaluation_pipeline(final_doc_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways\n",
    "\n",
    "+ Topic models (i.e. LDA) provide powerful tools to represent documents in a lower dimensional space\n",
    "\n",
    "+ Gensim package provides an LDA implementation based on online variational bayes (details in the paper: https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation)\n",
    "\n",
    "+ Some preprocess is often necessary/useful to \"normalize\" textual data (i.e. tokenization, stopwords filter, stemming, etc.)\n",
    "\n",
    "+ Generated topics can be qualitatively interpreted and evaluated through their most representative words\n",
    "\n",
    "+ Some coherence measurements based on PMI are introduced to automatically assess topics quality (more details here: http://qpleple.com/topic-coherence-to-evaluate-topic-models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
