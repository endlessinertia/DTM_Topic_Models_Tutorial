{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook created by: Gabriele Sottocornola\n",
    "for the M.Sc. class of Data & Text Mining\n",
    "'''\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_preprocessing(corpus_list):\n",
    "    '''\n",
    "    Function to apply some standard preprocessing to a list of documents in a corpus.\n",
    "    The preprocessing includes: split each document into a list of tokens, remove english stopwords,\n",
    "    remove alphanumerical tokens shorter that 3 chars.\n",
    "    Return a list of documents, each represented as a list of tokens\n",
    "    '''\n",
    "    #set tokenizer and stopwords filter\n",
    "    tokenizer = RegexpTokenizer('[–\\\\w\\\\+\\\\-\\\\*′α-ωΑ-Ω]+')\n",
    "    en_stop = stop_words.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    doc_tokens_list = list() #list of tokens that represent each document\n",
    "\n",
    "    for document in corpus_list:\n",
    "        lower_doc = document.lower() #set all the document string to lowercase\n",
    "        tokens = tokenizer.tokenize(lower_doc) #tokenize the document string\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop] #filter out english stopwords\n",
    "        final_tokens = [i for i in stopped_tokens if (len(i) > 2) and re.search('[a-zA-Z]', i)] #filter out tokens with len less than 3 and numbers\n",
    "\n",
    "        doc_tokens_list.append(final_tokens)\n",
    "        \n",
    "    return doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_doc_tokens(doc_tokens_list):\n",
    "    '''\n",
    "    Function to stem all the word tokens provided by the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of stemmed tokens\n",
    "    '''\n",
    "    stem_doc_tokens_list = list()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens_list]\n",
    "        stem_doc_tokens_list.append(stemmed_tokens)\n",
    "        \n",
    "    return stem_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words_list(doc_tokens_list, top_n_words=100, top_freq=0.0):\n",
    "    '''\n",
    "    Function to get the list of the top_n_words most frequent words inside the corpus.\n",
    "    Could filter out words based also on a relative frequency threshold.\n",
    "    Return a tuple with a list of the most frequent word labels and frequencies\n",
    "    '''\n",
    "    dictionary = corpora.dictionary.Dictionary(doc_tokens_list) #dictionary of words extracted from the corpus (list of tokens)\n",
    "    bow = dictionary.doc2bow([token for doc in doc_tokens_list for token in doc])\n",
    "    bow.sort(key=lambda x:x[1], reverse=True) #bow rapresentation of the dictionary sorted descending by frequency\n",
    "    \n",
    "    word_label_list = [dictionary[x[0]] for x in bow]\n",
    "    word_freq_list = [x[1] for x in bow]    \n",
    "    if top_freq:\n",
    "        total_freq = float(sum(word_freq_list))\n",
    "        word_freq_list = [freq / total_freq for freq in word_freq_list]\n",
    "        cum_freq_list = np.cumsum(word_freq_list)\n",
    "        cum_freq_list = cum_freq_list[cum_freq_list <= top_freq]\n",
    "        top_n_words = len(cum_freq_list)\n",
    "    \n",
    "    print('number of most common words to filter out: {}'.format(top_n_words))\n",
    "    word_label_list = word_label_list[:top_n_words]\n",
    "    word_freq_list = word_freq_list[:top_n_words]\n",
    "\n",
    "    return (word_label_list, word_freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_common_words(doc_tokens_list, list_filter_words):\n",
    "    '''\n",
    "    Function to filter out the most common words provided by list_filter_words from the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of filtered tokens\n",
    "    '''\n",
    "    filtered_doc_tokens_list = list()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:        \n",
    "        filtered_tokens_list = [token for token in tokens_list if not(token in list_filter_words)]\n",
    "        filtered_doc_tokens_list.append(filtered_tokens_list)\n",
    "    \n",
    "    return filtered_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(train_corpus, num_topics, label, mallet_path='/Users/gab/Documents/mallet-2.0.8/bin/mallet', test_corpus=None):\n",
    "    '''\n",
    "    Perform a topic modeling with num_topics on train corpus exploiting MalletLda wrapper. Perform\n",
    "    topic inference on unseen documents in test corpus.\n",
    "    See folder 'mallet_output' for results: doc_topic distribution and topic_word_weights\n",
    "    '''\n",
    "    # label or prefix to identify the specific mallet files for this run\n",
    "    label = './mallet_output/' + label\n",
    "    # dictionary of words extracted from the train corpus (list of tokens)\n",
    "    dictionary = corpora.dictionary.Dictionary(train_corpus)\n",
    "    # gensim bag-of-words representation of the train corpus\n",
    "    bow_train_corpus = [dictionary.doc2bow(tokens) for tokens in train_corpus]    \n",
    "    if test_corpus:\n",
    "        # gensim bag-of-words representation of the test corpus\n",
    "        bow_test_corpus = [dictionary.doc2bow(tokens) for tokens in test_corpus]\n",
    "\n",
    "    num_dict_words = len(dictionary)\n",
    "    print('number of word in the dictionary: {}'.format(num_dict_words))\n",
    "    LDA_model = LdaMallet(mallet_path, corpus=bow_train_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                          optimize_interval=10, iterations=1000, prefix=label)\n",
    "    if test_corpus:\n",
    "        doc_topic_test = LDA_model[bow_test_corpus]\n",
    "    create_topic_word_weights_file(LDA_model, num_topics, num_dict_words, label)\n",
    "    \n",
    "    return LDA_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_word_weights_file(LDA_model, num_topics, num_dict_words, label):\n",
    "    '''\n",
    "    Utility function to convert the topic word representation of LDA_model print_topic()\n",
    "    into the actual Mallet representation of the word weights for each topic and each word\n",
    "    in the dictionary.\n",
    "    It writes the topic_word_weights file into the mallet directory provided by label.\n",
    "    '''\n",
    "    tww_string = ''\n",
    "    for topic in range(num_topics):\n",
    "        topic_string = LDA_model.print_topic(topic, topn=num_dict_words)\n",
    "        topic_word_weight_list = topic_string.split('+')\n",
    "\n",
    "        for weight_word in topic_word_weight_list:\n",
    "            weight = weight_word.split('*')[0].strip()\n",
    "            word = weight_word.split('*')[1].strip()\n",
    "            word = word[1:-1]\n",
    "            tww_string += '{}\\t{}\\t{}\\n'.format(str(topic), word, str(weight))\n",
    "\n",
    "    with open(label + 'wordweights.txt', 'w', encoding='utf-8') as tww_file:\n",
    "        tww_file.write(tww_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topic_keys(topic_path, k=5):\n",
    "    '''\n",
    "    Given the path to the Mallet-like topickeys file,\n",
    "    return the list of k most relevant words for each topic\n",
    "    '''\n",
    "    topic_keys_str = ''.format('utf-8')\n",
    "    with open(topic_path) as topic_file:\n",
    "        topic_lines = topic_file.readlines()\n",
    "    for topic in topic_lines:\n",
    "        topic_tokens = topic.split('\\t')\n",
    "        \n",
    "        topic_id = topic_tokens[0]\n",
    "        topic_keys_str += 'TOPIC {}: '.format(topic_id)\n",
    "        word_tokens = topic_tokens[2].split(' ')\n",
    "        \n",
    "        for word in word_tokens[:k]:\n",
    "            topic_keys_str += word + ', '\n",
    "        topic_keys_str = topic_keys_str[:-2] + '\\n'\n",
    "        \n",
    "    return topic_keys_str\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS #\n",
    "NUM_TOPICS = 25\n",
    "\n",
    "MALLET_LABEL = 'AssociatedPress_{}topics_'.format(NUM_TOPICS)\n",
    "STEMMING = True\n",
    "#TOP_WORDS_FILTER = 10\n",
    "TOP_WORDS_FREQ = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read textual documents from file\n",
    "documents_path = './data/AssociatedPress.txt'\n",
    "with open(documents_path, 'r', encoding='utf-8') as doc_f:\n",
    "    corpus_list = doc_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize documents and remove stopwords\n",
    "doc_tokens_list = tokenization_preprocessing(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of most common words to filter out: 96\n"
     ]
    }
   ],
   "source": [
    "#stem word tokens and remove most frequent words\n",
    "if STEMMING:\n",
    "    stem_doc_tokens_list = stem_doc_tokens(doc_tokens_list)\n",
    "    frequent_words_list = get_common_words_list(stem_doc_tokens_list, top_freq=TOP_WORDS_FREQ)[0]\n",
    "    final_doc_tokens_list = filter_common_words(stem_doc_tokens_list, frequent_words_list)\n",
    "else:\n",
    "    frequent_words_list = get_common_words_list(doc_tokens_list, top_freq=TOP_WORDS_FREQ)[0]\n",
    "    final_doc_tokens_list = filter_common_words(doc_tokens_list, frequent_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word in the dictionary: 26980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.wrappers.ldamallet.LdaMallet at 0x108b4fd68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_modeling(final_doc_tokens_list, num_topics=NUM_TOPICS, label=MALLET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0: hospit, children, medic, health, diseas, drug, patient, doctor, care, mother\n",
      "TOPIC 1: dukaki, campaign, jackson, republican, candid, presidenti, poll, governor, primari, convent\n",
      "TOPIC 2: africa, african, black, mandela, india, white, independ, apartheid, indian, anc\n",
      "TOPIC 3: tax, money, fund, loan, pay, incom, bond, file, debt, corpor\n",
      "TOPIC 4: film, music, art, award, movi, record, perform, play, theater, star\n",
      "TOPIC 5: rule, law, judg, appeal, attorney, violat, lawyer, file, lawsuit, claim\n",
      "TOPIC 6: attorney, trial, investig, sentenc, prison, convict, judg, alleg, crime, prosecutor\n",
      "TOPIC 7: build, wine, earthquak, club, homeless, quak, center, popul, white, editor\n",
      "TOPIC 8: panama, network, noriega, broadcast, nbc, game, abc, televis, cb, haiti\n",
      "TOPIC 9: water, river, southern, mile, area, rain, coast, wind, high, north\n",
      "TOPIC 10: dollar, cent, oil, rose, point, index, exchang, late, higher, fell\n",
      "TOPIC 11: japan, farmer, agricultur, farm, china, japanes, product, food, ton, soybean\n",
      "TOPIC 12: air, plane, flight, airlin, pilot, ship, space, aircraft, airport, accid\n",
      "TOPIC 13: west, east, german, israel, germani, isra, palestinian, jewish, arab, peac\n",
      "TOPIC 14: continu, polici, chang, problem, talk, decis, major, econom, hope, agreement\n",
      "TOPIC 15: gorbachev, communist, reform, republ, opposit, power, minist, moscow, econom, human\n",
      "TOPIC 16: sale, worker, corp, contract, offer, share, employe, oper, announc, execut\n",
      "TOPIC 17: senat, budget, congress, rep, sen, legisl, republican, cut, congression, defens\n",
      "TOPIC 18: drug, mexico, cocain, arrest, agent, suspect, mexican, author, extradit, custom\n",
      "TOPIC 19: plant, environment, test, comput, wast, pollut, energi, chemic, research, water\n",
      "TOPIC 20: shot, death, car, die, man, victim, shoot, night, hospit, arrest\n",
      "TOPIC 21: reagan, summit, gorbachev, baker, europ, treati, washington, foreign, moscow, defens\n",
      "TOPIC 22: student, school, protest, church, demonstr, bomb, strike, cathol, build, march\n",
      "TOPIC 23: famili, don, live, life, mr, good, wife, thing, children, women\n",
      "TOPIC 24: iraq, war, troop, iraqi, kuwait, iran, rebel, saudi, armi, soldier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(show_topic_keys('./mallet_output/{}topickeys.txt'.format(MALLET_LABEL), k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
