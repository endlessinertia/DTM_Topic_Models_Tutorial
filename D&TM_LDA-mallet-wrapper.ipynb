{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notebook created by: Gabriele Sottocornola\n",
    "for the M.Sc. class of Data & Text Mining\n",
    "'''\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenization_preprocessing(corpus_list):\n",
    "    '''\n",
    "    Function to apply some standard preprocessing to a list of documents in a corpus.\n",
    "    The preprocessing includes: split each document into a list of tokens, remove english stopwords,\n",
    "    remove alphanumerical tokens shorter that 3 chars.\n",
    "    Return a list of documents, each represented as a list of tokens\n",
    "    '''\n",
    "    #set tokenizer and stopwords filter\n",
    "    tokenizer = RegexpTokenizer('[–\\\\w\\\\+\\\\-\\\\*′α-ωΑ-Ω]+')\n",
    "    en_stop = stop_words.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    doc_tokens_list = list() #list of tokens that represent each document\n",
    "\n",
    "    for document in corpus_list:\n",
    "        lower_doc = document.lower() #set all the document string to lowercase\n",
    "        tokens = tokenizer.tokenize(lower_doc) #tokenize the document string\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop] #filter out english stopwords\n",
    "        final_tokens = [i for i in stopped_tokens if (len(i) > 2) and re.search('[a-zA-Z]', i)] #filter out tokens with len less than 3 and numbers\n",
    "\n",
    "        doc_tokens_list.append(final_tokens)\n",
    "        \n",
    "    return doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_doc_tokens(doc_tokens_list):\n",
    "    '''\n",
    "    Function to stem all the word tokens provided by the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of stemmed tokens\n",
    "    '''\n",
    "    stem_doc_tokens_list = list()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens_list]\n",
    "        stem_doc_tokens_list.append(stemmed_tokens)\n",
    "        \n",
    "    return stem_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words_list(doc_tokens_list, top_n_words=100, top_freq=0.0):\n",
    "    '''\n",
    "    Function to get the list of the top_n_words most frequent words inside the corpus.\n",
    "    Could filter out words based also on a relative frequency threshold.\n",
    "    Return a tuple with a list of the most frequent word labels and frequencies\n",
    "    '''\n",
    "    dictionary = corpora.dictionary.Dictionary(doc_tokens_list) #dictionary of words extracted from the corpus (list of tokens)\n",
    "    bow = dictionary.doc2bow([token for doc in doc_tokens_list for token in doc])\n",
    "    bow.sort(key=lambda x:x[1], reverse=True) #bow rapresentation of the dictionary sorted descending by frequency\n",
    "    \n",
    "    word_label_list = [dictionary[x[0]] for x in bow]\n",
    "    word_freq_list = [x[1] for x in bow]    \n",
    "    if top_freq:\n",
    "        total_freq = float(sum(word_freq_list))\n",
    "        word_freq_list = [freq / total_freq for freq in word_freq_list]\n",
    "        cum_freq_list = np.cumsum(word_freq_list)\n",
    "        cum_freq_list = cum_freq_list[cum_freq_list <= top_freq]\n",
    "        top_n_words = len(cum_freq_list)\n",
    "    \n",
    "    print('number of most common words to filter out: {}'.format(top_n_words))\n",
    "    word_label_list = word_label_list[:top_n_words]\n",
    "    word_freq_list = word_freq_list[:top_n_words]\n",
    "\n",
    "    return (word_label_list, word_freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_common_words(doc_tokens_list, list_filter_words):\n",
    "    '''\n",
    "    Function to filter out the most common words provided by list_filter_words from the doc_tokens_list corpus.\n",
    "    Return a list of documents, each represented as a list of filtered tokens\n",
    "    '''\n",
    "    filtered_doc_tokens_list = list()\n",
    "    \n",
    "    for tokens_list in doc_tokens_list:        \n",
    "        filtered_tokens_list = [token for token in tokens_list if not(token in list_filter_words)]\n",
    "        filtered_doc_tokens_list.append(filtered_tokens_list)\n",
    "    \n",
    "    return filtered_doc_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topic_modeling(train_corpus, num_topics, label, mallet_path='C:\\\\Mallet\\\\mallet-2.0.8\\\\bin\\\\mallet', test_corpus=None):\n",
    "    '''\n",
    "    Perform a topic modeling with num_topics on train corpus exploiting MalletLda wrapper. Perform\n",
    "    topic inference on unseen documents in test corpus.\n",
    "    See folder 'mallet_output' for results: doc_topic distribution and topic_word_weights\n",
    "    '''\n",
    "    # label or prefix to identify the specific mallet files for this run\n",
    "    label = '.\\\\mallet_output\\\\' + label\n",
    "    # dictionary of words extracted from the train corpus (list of tokens)\n",
    "    dictionary = corpora.dictionary.Dictionary(train_corpus)\n",
    "    # gensim bag-of-words representation of the train corpus\n",
    "    bow_train_corpus = [dictionary.doc2bow(tokens) for tokens in train_corpus]    \n",
    "    if test_corpus:\n",
    "        # gensim bag-of-words representation of the test corpus\n",
    "        bow_test_corpus = [dictionary.doc2bow(tokens) for tokens in test_corpus]\n",
    "\n",
    "    num_dict_words = len(dictionary)\n",
    "    print('number of word in the dictionary: {}'.format(num_dict_words))\n",
    "    LDA_model = LdaMallet(mallet_path, corpus=bow_train_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                          optimize_interval=10, iterations=1000, prefix=label)\n",
    "    if test_corpus:\n",
    "        doc_topic_test = LDA_model[bow_test_corpus]\n",
    "    create_topic_word_weights_file(LDA_model, num_topics, num_dict_words, label)\n",
    "    \n",
    "    return LDA_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_topic_word_weights_file(LDA_model, num_topics, num_dict_words, label):\n",
    "    '''\n",
    "    Utility function to convert the topic word representation of LDA_model print_topic()\n",
    "    into the actual Mallet representation of the word weights for each topic and each word\n",
    "    in the dictionary.\n",
    "    It writes the topic_word_weights file into the mallet directory provided by label.\n",
    "    '''\n",
    "    tww_string = ''\n",
    "    for topic in range(num_topics):\n",
    "        topic_string = LDA_model.print_topic(topic, topn=num_dict_words)\n",
    "        topic_word_weight_list = topic_string.split('+')\n",
    "\n",
    "        for weight_word in topic_word_weight_list:\n",
    "            weight = weight_word.split('*')[0].strip()\n",
    "            word = weight_word.split('*')[1].strip()\n",
    "            word = word[1:-1]\n",
    "            tww_string += '{}\\t{}\\t{}\\n'.format(str(topic), word, str(weight))\n",
    "\n",
    "    with open(label + 'wordweights.txt', 'w', encoding='utf-8') as tww_file:\n",
    "        tww_file.write(tww_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_topic_keys(topic_path, k=5):\n",
    "    '''\n",
    "    Given the path to the Mallet-like topickeys file,\n",
    "    return the list of k most relevant words for each topic\n",
    "    '''\n",
    "    topic_keys_str = ''.format('utf-8')\n",
    "    with open(topic_path) as topic_file:\n",
    "        topic_lines = topic_file.readlines()\n",
    "    for topic in topic_lines:\n",
    "        topic_tokens = topic.split('\\t')\n",
    "        \n",
    "        topic_id = topic_tokens[0]\n",
    "        topic_keys_str += 'TOPIC {}: '.format(topic_id)\n",
    "        word_tokens = topic_tokens[2].split(' ')\n",
    "        \n",
    "        for word in word_tokens[:k]:\n",
    "            topic_keys_str += word + ', '\n",
    "        topic_keys_str = topic_keys_str[:-2] + '\\n'\n",
    "        \n",
    "    return topic_keys_str\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PARAMETERS #\n",
    "NUM_TOPICS = 20\n",
    "MALLET_LABEL = 'AssociatedPress_5topics_'\n",
    "STEMMING = False\n",
    "TOP_WORDS_FILTER = 50\n",
    "TOP_WORDS_FREQ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read textual documents from file\n",
    "documents_path = '.\\\\data\\\\AssociatedPress.txt'\n",
    "with open(documents_path, 'r', encoding='utf-8') as doc_f:\n",
    "    corpus_list = doc_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize documents and remove stopwords\n",
    "doc_tokens_list = tokenization_preprocessing(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of most common words to filter out: 35\n"
     ]
    }
   ],
   "source": [
    "#stem word tokens and remove most frequent words\n",
    "if STEMMING:\n",
    "    stem_doc_tokens_list = stem_doc_tokens(doc_tokens_list)\n",
    "    frequent_words_list = get_common_words_list(stem_doc_tokens_list, top_freq=TOP_WORDS_FREQ)[0]\n",
    "    final_doc_tokens_list = filter_common_words(stem_doc_tokens_list, frequent_words_list)\n",
    "else:\n",
    "    frequent_words_list = get_common_words_list(doc_tokens_list, top_freq=TOP_WORDS_FREQ)[0]\n",
    "    final_doc_tokens_list = filter_common_words(doc_tokens_list, frequent_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word in the dictionary: 37734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.wrappers.ldamallet.LdaMallet at 0x126a217ae10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_modeling(final_doc_tokens_list, num_topics=NUM_TOPICS, label=MALLET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0: dukakis, reagan, committee, campaign, congress, administration\n",
      "TOPIC 1: market, prices, stock, oil, dollar, trade\n",
      "TOPIC 2: military, gorbachev, minister, south, war, official\n",
      "TOPIC 3: air, miles, water, area, space, flight\n",
      "TOPIC 4: children, school, family, man, prison, death\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(show_topic_keys('.\\\\mallet_output\\\\AssociatedPress_5topics_topickeys.txt', k=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
